{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers, losses\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, Cropping2D\n",
    "\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_concepts.txt', delimiter=\":\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0738276</td>\n",
       "      <td>btt-8-083Fig3;abcd-28-01-0087-g03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1287716</td>\n",
       "      <td>ipej030157-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0576088</td>\n",
       "      <td>aps-43-590-g001;aps-43-590-g002;aps-43-590-g00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C2332719</td>\n",
       "      <td>CRIRH2016-2019250.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0796089</td>\n",
       "      <td>kjlm-31-49-g001;MGG3-4-599-g001;JPN-6-19-g004;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0                                                  1\n",
       "0  C0738276                  btt-8-083Fig3;abcd-28-01-0087-g03\n",
       "1  C1287716                                      ipej030157-10\n",
       "2  C0576088  aps-43-590-g001;aps-43-590-g002;aps-43-590-g00...\n",
       "3  C2332719                              CRIRH2016-2019250.001\n",
       "4  C0796089  kjlm-31-49-g001;MGG3-4-599-g001;JPN-6-19-g004;..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for index, row in data[:30].iterrows():\n",
    "#     print index\n",
    "    for i in row[1].split(';'):\n",
    "#         print str(i)+';'+str(row[0])+';'+str(index)\n",
    "        res.append(str(i)+';'+str(row[0])+';'+str(index))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                0\n",
      "0        btt-8-083Fig3;C0738276;0\n",
      "1  abcd-28-01-0087-g03;C0738276;0\n",
      "2        ipej030157-10;C1287716;1\n",
      "3      aps-43-590-g001;C0576088;2\n",
      "4      aps-43-590-g002;C0576088;2\n"
     ]
    }
   ],
   "source": [
    "final_results = pd.DataFrame(\n",
    "    {\n",
    "        \"0\":res,\n",
    "    })\n",
    "print final_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_results.to_csv('Final_prediction1000.csv',  index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424 1424\n"
     ]
    }
   ],
   "source": [
    "now = pd.read_csv('/Users/sharath/Desktop/GP/sharath/Final_prediction1000.csv', delimiter=\";\", header=None)\n",
    "k = now[[2]].values\n",
    "res = np_utils.to_categorical(k)\n",
    "# print res[:5]\n",
    "leng =  res.shape[1]\n",
    "print len(now[0]), len(res)\n",
    "\n",
    "dataset_size  = len(now[0])\n",
    "\n",
    "# print now.head()\n",
    "\n",
    "labels = pd.DataFrame(res)\n",
    "# print labels.head()\n",
    "\n",
    "\n",
    "result = pd.concat([now, labels], axis=1, join_axes=[now.index])\n",
    "# print result.head()\n",
    "# result.to_csv(\"preprocessed.txt\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgdim = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharath/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(80, 80, 3...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 80, 80, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               25600500  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                15030     \n",
      "=================================================================\n",
      "Total params: 25,616,426\n",
      "Trainable params: 25,616,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape = (imgdim, imgdim,3),border_mode = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(leng, activation = 'softmax')) \n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(64,64,3)))\n",
    "# model.add(Conv2D(32, (5, 5), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "# model.add(Flatten())\n",
    "# # model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0\n",
      "(1000, 64, 64, 3) (1000, 30)\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 13.5217 - acc: 0.1610\n"
     ]
    }
   ],
   "source": [
    "def BatchGenerator(track_index):\n",
    "    track_index = track_index\n",
    "    X = []\n",
    "    y = []\n",
    "    print  track_index\n",
    "    for index, row in result[track_index:track_index+plus_track_index].iterrows():\n",
    "#         print row.index\n",
    "        x = row.values[0]\n",
    "        label = row.values[3:]\n",
    "        x = row.values[0]\n",
    "        y.append(label)\n",
    "        img = Image.open(\"/Users/sharath/Desktop/GP/CaptionTraining2018/\"+ x +\".jpg\")\n",
    "        img = img.resize((imgdim,imgdim)).convert('RGB')\n",
    "        T = (np.array(img))\n",
    "        X.append(T)\n",
    "#         track_index =  index\n",
    "    X_train = np.array(X)\n",
    "    y_train = np.array(y)\n",
    "#     print X_train.shape, y_train.shape\n",
    "    return X_train, y_train\n",
    "    \n",
    "\n",
    "nb_epoch = 1\n",
    "track_index = 0\n",
    "# plus_track_index = (dataset_size/10)\n",
    "plus_track_index = 1000\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    print(\"epoch %d\" % e)\n",
    "    X,y = BatchGenerator(track_index) \n",
    "    print X.shape,y.shape\n",
    "    model.fit(X, y)\n",
    "    track_index = track_index + plus_track_index\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# # compute quantities required for featurewise normalization\n",
    "# # (std, mean, and principal components if ZCA whitening is applied)\n",
    "# datagen.fit(x_train)\n",
    "\n",
    "# # # fits the model on batches with real-time data augmentation:\n",
    "# # model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "# #                     steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
    "\n",
    "# # here's a more \"manual\" example\n",
    "# for e in range(epochs):\n",
    "#     print('Epoch', e)\n",
    "#     batches = 0\n",
    "#     for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
    "#         model.fit(x_batch, y_batch)\n",
    "#         batches += 1\n",
    "#         if batches >= len(x_train) / 32:\n",
    "#             # we need to break the loop by hand because\n",
    "#             # the generator loops indefinitely\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # def generate_arrays_from_file():\n",
    "    \n",
    "#     while 1:\n",
    "#         f = open(path)\n",
    "#         for line in f:\n",
    "#             # create numpy arrays of input data\n",
    "#             # and labels, from each line in the file\n",
    "#             x, y = process_line(line)\n",
    "#             img = load_images(x)\n",
    "#             yield (img, y)\n",
    "#         f.close()\n",
    "\n",
    "        \n",
    "# model.fit_generator(generate_arrays_from_file(),steps_per_epoch='10',epochs=10)\n",
    "\n",
    "\n",
    "# log_dir = './tf-log/'\n",
    "# tb_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# cbks = [tb_cb]\n",
    "\n",
    "# target_dir = './models/'\n",
    "# if not os.path.exists(target_dir):\n",
    "#   os.mkdir(target_dir)\n",
    "# model.save('./models/model.h5')\n",
    "# model.save_weights('./models/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def generate_arrays_from_file():\n",
    "#     for index, row in now.iterrows():\n",
    "#         print index\n",
    "#         x = row.values[0]\n",
    "#         y = res[index]\n",
    "#         img = Image.open(\"/Users/sharath/Desktop/GP/CaptionTraining2018/\"+ x +\".jpg\")\n",
    "#         T.append(np.array(img))\n",
    "#         X = np.array(T)\n",
    "#     yield (X, y)\n",
    "    \n",
    "# model.fit_generator(generate_arrays_from_file(),steps_per_epoch='1',epochs=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# log_dir = './tf-log/'\n",
    "# tb_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# cbks = [tb_cb]\n",
    "\n",
    "# target_dir = './models/'\n",
    "# if not os.path.exists(target_dir):\n",
    "#   os.mkdir(target_dir)\n",
    "# model.save('./models/model.h5')\n",
    "# model.save_weights('./models/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
